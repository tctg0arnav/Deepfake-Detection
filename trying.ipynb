{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluates a folder of video files or a single file with a xception binary\n",
    "classification network.\n",
    "\n",
    "Usage:\n",
    "python detect_from_video.py\n",
    "    -i <folder with video files or path to video file>\n",
    "    -m <path to model file>\n",
    "    -o <path to output folder, will write one or multiple output videos there>\n",
    "\n",
    "Author: Andreas RÃ¶ssler\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from os.path import join\n",
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image as pil_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from network.models import model_selection\n",
    "from dataset.transform import xception_default_data_transforms\n",
    "\n",
    "\n",
    "def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n",
    "    \"\"\"\n",
    "    Expects a dlib face to generate a quadratic bounding box.\n",
    "    :param face: dlib face class\n",
    "    :param width: frame width\n",
    "    :param height: frame height\n",
    "    :param scale: bounding box size multiplier to get a bigger face region\n",
    "    :param minsize: set minimum bounding box size\n",
    "    :return: x, y, bounding_box_size in opencv form\n",
    "    \"\"\"\n",
    "    x1 = face.left()\n",
    "    y1 = face.top()\n",
    "    x2 = face.right()\n",
    "    y2 = face.bottom()\n",
    "    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n",
    "    if minsize:\n",
    "        if size_bb < minsize:\n",
    "            size_bb = minsize\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "    # Check for out of bounds, x-y top left corner\n",
    "    x1 = max(int(center_x - size_bb // 2), 0)\n",
    "    y1 = max(int(center_y - size_bb // 2), 0)\n",
    "    # Check for too big bb size for given x, y\n",
    "    size_bb = min(width - x1, size_bb)\n",
    "    size_bb = min(height - y1, size_bb)\n",
    "\n",
    "    return x1, y1, size_bb\n",
    "\n",
    "\n",
    "def preprocess_image(image, cuda=False):\n",
    "    \"\"\"\n",
    "    Preprocesses the image such that it can be fed into our network.\n",
    "    During this process we envoke PIL to cast it into a PIL image.\n",
    "\n",
    "    :param image: numpy image in opencv form (i.e., BGR and of shape\n",
    "    :return: pytorch tensor of shape [1, 3, image_size, image_size], not\n",
    "    necessarily casted to cuda\n",
    "    \"\"\"\n",
    "    # Revert from BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Preprocess using the preprocessing function used during training and\n",
    "    # casting it to PIL image\n",
    "    preprocess = xception_default_data_transforms[\"test\"]\n",
    "    preprocessed_image = preprocess(pil_image.fromarray(image))\n",
    "    # Add first dimension as the network expects a batch\n",
    "    preprocessed_image = preprocessed_image.unsqueeze(0)\n",
    "    if cuda:\n",
    "        preprocessed_image = preprocessed_image.cuda()\n",
    "    return preprocessed_image\n",
    "\n",
    "\n",
    "def predict_with_model(image, model, post_function=nn.Softmax(dim=1), cuda=False):\n",
    "    \"\"\"\n",
    "    Predicts the label of an input image. Preprocesses the input image and\n",
    "    casts it to cuda if required\n",
    "\n",
    "    :param image: numpy image\n",
    "    :param model: torch model with linear layer at the end\n",
    "    :param post_function: e.g., softmax\n",
    "    :param cuda: enables cuda, must be the same parameter as the model\n",
    "    :return: prediction (1 = fake, 0 = real)\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    preprocessed_image = preprocess_image(image, cuda)\n",
    "\n",
    "    # Model prediction\n",
    "    output = model(preprocessed_image)\n",
    "    output = post_function(output)\n",
    "\n",
    "    # Cast to desired\n",
    "    _, prediction = torch.max(output, 1)  # argmax\n",
    "    prediction = float(prediction.cpu().numpy())\n",
    "\n",
    "    return int(prediction), output\n",
    "\n",
    "\n",
    "def test_full_image_network(\n",
    "    video_path, model_path, output_path, start_frame=0, end_frame=None, cuda=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads a video and evaluates a subset of frames with the a detection network\n",
    "    that takes in a full frame. Outputs are only given if a face is present\n",
    "    and the face is highlighted using dlib.\n",
    "    :param video_path: path to video file\n",
    "    :param model_path: path to model file (should expect the full sized image)\n",
    "    :param output_path: path where the output video is stored\n",
    "    :param start_frame: first frame to evaluate\n",
    "    :param end_frame: last frame to evaluate\n",
    "    :param cuda: enable cuda\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"Starting: {}\".format(video_path))\n",
    "\n",
    "    # Read and write\n",
    "    reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "    video_fn = video_path.split(\"/\")[-1].split(\".\")[0] + \".avi\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "    fps = reader.get(cv2.CAP_PROP_FPS)\n",
    "    num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    writer = None\n",
    "\n",
    "    # Face detector\n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # Load model\n",
    "    model = model_selection(modelname=\"xception\", num_out_classes=2, dropout=0.5)\n",
    "    model.load_state_dict(torch.load(model_path), map_location=torch.device(\"cpu\"))\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model = model.module\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Text variables\n",
    "    font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    thickness = 2\n",
    "    font_scale = 1\n",
    "\n",
    "    # Frame numbers and length of output video\n",
    "    frame_num = 0\n",
    "    assert start_frame < num_frames - 1\n",
    "    end_frame = end_frame if end_frame else num_frames\n",
    "    pbar = tqdm(total=end_frame - start_frame)\n",
    "\n",
    "    while reader.isOpened():\n",
    "        _, image = reader.read()\n",
    "        if image is None:\n",
    "            break\n",
    "        frame_num += 1\n",
    "\n",
    "        if frame_num < start_frame:\n",
    "            continue\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Image size\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        # Init output writer\n",
    "        if writer is None:\n",
    "            writer = cv2.VideoWriter(\n",
    "                join(output_path, video_fn), fourcc, fps, (height, width)[::-1]\n",
    "            )\n",
    "\n",
    "        # 2. Detect with dlib\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detector(gray, 1)\n",
    "        if len(faces):\n",
    "            # For now only take biggest face\n",
    "            face = faces[0]\n",
    "\n",
    "            # --- Prediction ---------------------------------------------------\n",
    "            # Face crop with dlib and bounding box scale enlargement\n",
    "            x, y, size = get_boundingbox(face, width, height)\n",
    "            cropped_face = image[y : y + size, x : x + size]\n",
    "\n",
    "            # Actual prediction using our model\n",
    "            prediction, output = predict_with_model(cropped_face, model, cuda=cuda)\n",
    "            # ------------------------------------------------------------------\n",
    "\n",
    "            # Text and bb\n",
    "            x = face.left()\n",
    "            y = face.top()\n",
    "            w = face.right() - x\n",
    "            h = face.bottom() - y\n",
    "            label = \"fake\" if prediction == 1 else \"real\"\n",
    "            color = (0, 255, 0) if prediction == 0 else (0, 0, 255)\n",
    "            output_list = [\n",
    "                \"{0:.2f}\".format(float(x)) for x in output.detach().cpu().numpy()[0]\n",
    "            ]\n",
    "            cv2.putText(\n",
    "                image,\n",
    "                str(output_list) + \"=>\" + label,\n",
    "                (x, y + h + 30),\n",
    "                font_face,\n",
    "                font_scale,\n",
    "                color,\n",
    "                thickness,\n",
    "                2,\n",
    "            )\n",
    "            # draw box over face\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "        if frame_num >= end_frame:\n",
    "            break\n",
    "\n",
    "        # Show\n",
    "        cv2.imshow(\"test\", image)\n",
    "        cv2.waitKey(33)  # About 30 fps\n",
    "        writer.write(image)\n",
    "    pbar.close()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "        print(\"Finished! Output saved under {}\".format(output_path))\n",
    "    else:\n",
    "        print(\"Input video file was empty\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    p.add_argument(\"--video_path\", \"-i\", type=str)\n",
    "    p.add_argument(\"--model_path\", \"-mi\", type=str, default=None)\n",
    "    p.add_argument(\"--output_path\", \"-o\", type=str, default=\".\")\n",
    "    p.add_argument(\"--start_frame\", type=int, default=0)\n",
    "    p.add_argument(\"--end_frame\", type=int, default=None)\n",
    "    p.add_argument(\"--cuda\", action=\"store_true\")\n",
    "    args = p.parse_args()\n",
    "\n",
    "    video_path = args.video_path\n",
    "    if video_path.endswith(\".mp4\") or video_path.endswith(\".avi\"):\n",
    "        test_full_image_network(**vars(args))\n",
    "    else:\n",
    "        videos = os.listdir(video_path)\n",
    "        for video in videos:\n",
    "            args.video_path = join(video_path, video)\n",
    "            test_full_image_network(**vars(args))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
